{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN on CIFR Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use Dense Layers (also called fully connected layers), or DropOut.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from keras import models, layers\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import cifar10\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "#batch_size = 64\n",
    "num_classes = 10\n",
    "l = 6\n",
    "num_filter = 32\n",
    "compression = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "      \n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    \n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    c=layers.Conv2D(10, (1, 1), padding='valid')(AvgPooling)\n",
    "\n",
    "    avg=layers.GlobalAveragePooling2D()(c)\n",
    "    \n",
    "    output=layers.Activation('softmax')(avg)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter)\n",
    "First_Transition = transition(First_Block, num_filter)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter)\n",
    "Second_Transition = transition(Second_Block, num_filter)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter)\n",
    "Third_Transition = transition(Third_Block, num_filter)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 32, 32, 32)   864         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 32, 32, 32)   128         conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 32, 32, 32)   0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 32, 32, 32)   9216        activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_289 (Concatenate)   (None, 32, 32, 64)   0           conv2d_335[0][0]                 \n",
      "                                                                 conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 32, 32, 64)   256         concatenate_289[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 32, 32, 64)   0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 32, 32, 32)   18432       activation_336[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_290 (Concatenate)   (None, 32, 32, 96)   0           concatenate_289[0][0]            \n",
      "                                                                 conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 32, 32, 96)   384         concatenate_290[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 32, 32, 96)   0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 32, 32, 32)   27648       activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_291 (Concatenate)   (None, 32, 32, 128)  0           concatenate_290[0][0]            \n",
      "                                                                 conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 32, 32, 128)  512         concatenate_291[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 32, 32, 128)  0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 32, 32, 32)   36864       activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_292 (Concatenate)   (None, 32, 32, 160)  0           concatenate_291[0][0]            \n",
      "                                                                 conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 32, 32, 160)  640         concatenate_292[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 32, 32, 160)  0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 32, 32, 32)   46080       activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_293 (Concatenate)   (None, 32, 32, 192)  0           concatenate_292[0][0]            \n",
      "                                                                 conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 32, 32, 192)  768         concatenate_293[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 32, 32, 192)  0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 32, 32, 32)   55296       activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_294 (Concatenate)   (None, 32, 32, 224)  0           concatenate_293[0][0]            \n",
      "                                                                 conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 32, 32, 224)  896         concatenate_294[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 32, 32, 224)  0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 32, 32, 32)   7168        activation_341[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_41 (AveragePo (None, 16, 16, 32)   0           conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 16, 16, 32)   128         average_pooling2d_41[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 16, 16, 32)   0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 16, 16, 32)   9216        activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_295 (Concatenate)   (None, 16, 16, 64)   0           average_pooling2d_41[0][0]       \n",
      "                                                                 conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 16, 16, 64)   256         concatenate_295[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 16, 16, 64)   0           batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 16, 16, 32)   18432       activation_343[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_296 (Concatenate)   (None, 16, 16, 96)   0           concatenate_295[0][0]            \n",
      "                                                                 conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_338 (BatchN (None, 16, 16, 96)   384         concatenate_296[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 16, 16, 96)   0           batch_normalization_338[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 16, 16, 32)   27648       activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_297 (Concatenate)   (None, 16, 16, 128)  0           concatenate_296[0][0]            \n",
      "                                                                 conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 16, 16, 128)  512         concatenate_297[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 16, 16, 128)  0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 16, 16, 32)   36864       activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_298 (Concatenate)   (None, 16, 16, 160)  0           concatenate_297[0][0]            \n",
      "                                                                 conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 16, 16, 160)  640         concatenate_298[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 16, 16, 160)  0           batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 16, 16, 32)   46080       activation_346[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_299 (Concatenate)   (None, 16, 16, 192)  0           concatenate_298[0][0]            \n",
      "                                                                 conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_341 (BatchN (None, 16, 16, 192)  768         concatenate_299[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 16, 16, 192)  0           batch_normalization_341[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 16, 16, 32)   55296       activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_300 (Concatenate)   (None, 16, 16, 224)  0           concatenate_299[0][0]            \n",
      "                                                                 conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 16, 16, 224)  896         concatenate_300[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 16, 16, 224)  0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 16, 16, 32)   7168        activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_42 (AveragePo (None, 8, 8, 32)     0           conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 8, 8, 32)     128         average_pooling2d_42[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 8, 8, 32)     0           batch_normalization_343[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 8, 8, 32)     9216        activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_301 (Concatenate)   (None, 8, 8, 64)     0           average_pooling2d_42[0][0]       \n",
      "                                                                 conv2d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 8, 8, 64)     256         concatenate_301[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 8, 8, 64)     0           batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 8, 8, 32)     18432       activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_302 (Concatenate)   (None, 8, 8, 96)     0           concatenate_301[0][0]            \n",
      "                                                                 conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_345 (BatchN (None, 8, 8, 96)     384         concatenate_302[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_351 (Activation)     (None, 8, 8, 96)     0           batch_normalization_345[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 8, 8, 32)     27648       activation_351[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_303 (Concatenate)   (None, 8, 8, 128)    0           concatenate_302[0][0]            \n",
      "                                                                 conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 8, 8, 128)    512         concatenate_303[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_352 (Activation)     (None, 8, 8, 128)    0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 8, 8, 32)     36864       activation_352[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_304 (Concatenate)   (None, 8, 8, 160)    0           concatenate_303[0][0]            \n",
      "                                                                 conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 8, 8, 160)    640         concatenate_304[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_353 (Activation)     (None, 8, 8, 160)    0           batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 8, 8, 32)     46080       activation_353[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_305 (Concatenate)   (None, 8, 8, 192)    0           concatenate_304[0][0]            \n",
      "                                                                 conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_348 (BatchN (None, 8, 8, 192)    768         concatenate_305[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_354 (Activation)     (None, 8, 8, 192)    0           batch_normalization_348[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 8, 8, 32)     55296       activation_354[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_306 (Concatenate)   (None, 8, 8, 224)    0           concatenate_305[0][0]            \n",
      "                                                                 conv2d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 8, 8, 224)    896         concatenate_306[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_355 (Activation)     (None, 8, 8, 224)    0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 8, 8, 32)     7168        activation_355[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_43 (AveragePo (None, 4, 4, 32)     0           conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 4, 4, 32)     128         average_pooling2d_43[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_356 (Activation)     (None, 4, 4, 32)     0           batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 4, 4, 32)     9216        activation_356[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_307 (Concatenate)   (None, 4, 4, 64)     0           average_pooling2d_43[0][0]       \n",
      "                                                                 conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_351 (BatchN (None, 4, 4, 64)     256         concatenate_307[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_357 (Activation)     (None, 4, 4, 64)     0           batch_normalization_351[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 4, 4, 32)     18432       activation_357[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_308 (Concatenate)   (None, 4, 4, 96)     0           concatenate_307[0][0]            \n",
      "                                                                 conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_352 (BatchN (None, 4, 4, 96)     384         concatenate_308[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_358 (Activation)     (None, 4, 4, 96)     0           batch_normalization_352[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 4, 4, 32)     27648       activation_358[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_309 (Concatenate)   (None, 4, 4, 128)    0           concatenate_308[0][0]            \n",
      "                                                                 conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_353 (BatchN (None, 4, 4, 128)    512         concatenate_309[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_359 (Activation)     (None, 4, 4, 128)    0           batch_normalization_353[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 4, 4, 32)     36864       activation_359[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_310 (Concatenate)   (None, 4, 4, 160)    0           concatenate_309[0][0]            \n",
      "                                                                 conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_354 (BatchN (None, 4, 4, 160)    640         concatenate_310[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_360 (Activation)     (None, 4, 4, 160)    0           batch_normalization_354[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 4, 4, 32)     46080       activation_360[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_311 (Concatenate)   (None, 4, 4, 192)    0           concatenate_310[0][0]            \n",
      "                                                                 conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_355 (BatchN (None, 4, 4, 192)    768         concatenate_311[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_361 (Activation)     (None, 4, 4, 192)    0           batch_normalization_355[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 4, 4, 32)     55296       activation_361[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_312 (Concatenate)   (None, 4, 4, 224)    0           concatenate_311[0][0]            \n",
      "                                                                 conv2d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_356 (BatchN (None, 4, 4, 224)    896         concatenate_312[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_362 (Activation)     (None, 4, 4, 224)    0           batch_normalization_356[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_44 (AveragePo (None, 2, 2, 224)    0           activation_362[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 2, 2, 10)     2250        average_pooling2d_44[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 10)           0           conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_363 (Activation)     (None, 10)           0           global_average_pooling2d_7[0][0] \n",
      "==================================================================================================\n",
      "Total params: 813,098\n",
      "Trainable params: 805,930\n",
      "Non-trainable params: 7,168\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"best_model.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath,monitor='val_acc',verbose=1,save_best_only=True,mode='max')\n",
    "callbacks_list =[checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "\n",
    "# https://medium.com/@arindambaidya168/https-medium-com-arindambaidya168-using-keras-imagedatagenerator-b94a87cdefad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=30,\n",
    "                             zoom_range=0.1,\n",
    "                             width_shift_range=0.15, \n",
    "                             height_shift_range=0.15, \n",
    "                             horizontal_flip=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "781/781 [==============================] - 7165s 9s/step - loss: 1.6435 - acc: 0.3934 - val_loss: 2.3846 - val_acc: 0.3026\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30260, saving model to best_model.hdf5\n",
      "Epoch 2/25\n",
      "781/781 [==============================] - 7254s 9s/step - loss: 1.3133 - acc: 0.5272 - val_loss: 1.2857 - val_acc: 0.5548\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.30260 to 0.55480, saving model to best_model.hdf5\n",
      "Epoch 3/25\n",
      "781/781 [==============================] - 7230s 9s/step - loss: 1.1488 - acc: 0.5903 - val_loss: 1.4544 - val_acc: 0.5106\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.55480\n",
      "Epoch 4/25\n",
      "781/781 [==============================] - 7245s 9s/step - loss: 1.0399 - acc: 0.6317 - val_loss: 0.9488 - val_acc: 0.6618\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.55480 to 0.66180, saving model to best_model.hdf5\n",
      "Epoch 5/25\n",
      "781/781 [==============================] - 7159s 9s/step - loss: 0.9630 - acc: 0.6600 - val_loss: 0.9395 - val_acc: 0.6792\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.66180 to 0.67920, saving model to best_model.hdf5\n",
      "Epoch 6/25\n",
      "781/781 [==============================] - 7160s 9s/step - loss: 0.8912 - acc: 0.6893 - val_loss: 1.0140 - val_acc: 0.6499\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.67920\n",
      "Epoch 7/25\n",
      "781/781 [==============================] - 7147s 9s/step - loss: 0.8450 - acc: 0.7035 - val_loss: 0.9780 - val_acc: 0.6849\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.67920 to 0.68490, saving model to best_model.hdf5\n",
      "Epoch 8/25\n",
      "781/781 [==============================] - 7145s 9s/step - loss: 0.7987 - acc: 0.7220 - val_loss: 1.0113 - val_acc: 0.6636\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.68490\n",
      "Epoch 9/25\n",
      "781/781 [==============================] - 7147s 9s/step - loss: 0.7581 - acc: 0.7389 - val_loss: 0.8447 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.68490 to 0.71700, saving model to best_model.hdf5\n",
      "Epoch 10/25\n",
      "781/781 [==============================] - 7143s 9s/step - loss: 0.7334 - acc: 0.7468 - val_loss: 0.8250 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.71700 to 0.72190, saving model to best_model.hdf5\n",
      "Epoch 11/25\n",
      "781/781 [==============================] - 7133s 9s/step - loss: 0.6961 - acc: 0.7591 - val_loss: 0.9963 - val_acc: 0.6902\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.72190\n",
      "Epoch 12/25\n",
      "781/781 [==============================] - 7139s 9s/step - loss: 0.6828 - acc: 0.7640 - val_loss: 0.7899 - val_acc: 0.7369\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.72190 to 0.73690, saving model to best_model.hdf5\n",
      "Epoch 13/25\n",
      "781/781 [==============================] - 7193s 9s/step - loss: 0.6551 - acc: 0.7702 - val_loss: 0.6637 - val_acc: 0.7662\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.73690 to 0.76620, saving model to best_model.hdf5\n",
      "Epoch 14/25\n",
      "781/781 [==============================] - 7196s 9s/step - loss: 0.6357 - acc: 0.7795 - val_loss: 0.7861 - val_acc: 0.7473\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76620\n",
      "Epoch 15/25\n",
      "781/781 [==============================] - 7145s 9s/step - loss: 0.6164 - acc: 0.7854 - val_loss: 0.9713 - val_acc: 0.6894\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76620\n",
      "Epoch 16/25\n",
      "781/781 [==============================] - 7158s 9s/step - loss: 0.6046 - acc: 0.7921 - val_loss: 0.6712 - val_acc: 0.7784\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.76620 to 0.77840, saving model to best_model.hdf5\n",
      "Epoch 17/25\n",
      "781/781 [==============================] - 7172s 9s/step - loss: 0.5674 - acc: 0.8045 - val_loss: 0.6210 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.77840 to 0.78670, saving model to best_model.hdf5\n",
      "Epoch 18/25\n",
      "781/781 [==============================] - 7941s 10s/step - loss: 0.5767 - acc: 0.7999 - val_loss: 0.7381 - val_acc: 0.7558\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.78670\n",
      "Epoch 19/25\n",
      "781/781 [==============================] - 7810s 10s/step - loss: 0.5482 - acc: 0.8082 - val_loss: 0.6295 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.78670 to 0.79200, saving model to best_model.hdf5\n",
      "Epoch 20/25\n",
      "781/781 [==============================] - 7157s 9s/step - loss: 0.5533 - acc: 0.8080 - val_loss: 0.5924 - val_acc: 0.8008\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.79200 to 0.80080, saving model to best_model.hdf5\n",
      "Epoch 21/25\n",
      "634/781 [=======================>......] - ETA: 21:28 - loss: 0.5374 - acc: 0.8138"
     ]
    }
   ],
   "source": [
    "datagen.fit(X_train)\n",
    "\n",
    "# fit the model on the batches generated by datagen.flow()\n",
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=X_train.shape[0] // 64,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=25, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training from 21st epoch as laptop was frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models    \n",
    "model = models.load_model('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"best_model_r2.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath,monitor='val_acc',verbose=1,save_best_only=True,mode='max')\n",
    "callbacks_list =[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "781/781 [==============================] - 7145s 9s/step - loss: 0.5239 - acc: 0.8199 - val_loss: 0.5421 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00021: val_acc improved from -inf to 0.81730, saving model to best_model_r2.hdf5\n",
      "Epoch 22/30\n",
      "781/781 [==============================] - 7452s 10s/step - loss: 0.5266 - acc: 0.8179 - val_loss: 0.7297 - val_acc: 0.7637\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.81730\n",
      "Epoch 23/30\n",
      "781/781 [==============================] - 7433s 10s/step - loss: 0.5037 - acc: 0.8254 - val_loss: 0.6858 - val_acc: 0.7783\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.81730\n",
      "Epoch 24/30\n",
      "781/781 [==============================] - 7820s 10s/step - loss: 0.4921 - acc: 0.8298 - val_loss: 0.6768 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.81730\n",
      "Epoch 25/30\n",
      "781/781 [==============================] - 8927s 11s/step - loss: 0.4948 - acc: 0.8304 - val_loss: 0.6798 - val_acc: 0.7927\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.81730\n",
      "Epoch 26/30\n",
      "781/781 [==============================] - 8208s 11s/step - loss: 0.4704 - acc: 0.8381 - val_loss: 0.5429 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.81730 to 0.81970, saving model to best_model_r2.hdf5\n",
      "Epoch 27/30\n",
      "781/781 [==============================] - 7792s 10s/step - loss: 0.4614 - acc: 0.8396 - val_loss: 0.6292 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81970\n",
      "Epoch 28/30\n",
      "781/781 [==============================] - 8534s 11s/step - loss: 0.4662 - acc: 0.8383 - val_loss: 0.5885 - val_acc: 0.8146\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81970\n",
      "Epoch 29/30\n",
      "781/781 [==============================] - 8264s 11s/step - loss: 0.4506 - acc: 0.8438 - val_loss: 0.5055 - val_acc: 0.8348\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.81970 to 0.83480, saving model to best_model_r2.hdf5\n",
      "Epoch 30/30\n",
      "781/781 [==============================] - 7178s 9s/step - loss: 0.4537 - acc: 0.8440 - val_loss: 0.5940 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83480\n"
     ]
    }
   ],
   "source": [
    "datagen.fit(X_train)\n",
    "\n",
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=X_train.shape[0] // 64,\n",
    "                        validation_data=(X_test, y_test),initial_epoch=20,\n",
    "                        epochs=30, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from keras import models    \n",
    "model = models.load_model('best_model_r2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"best_model_r3.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath,monitor='val_acc',verbose=1,save_best_only=True,mode='max')\n",
    "callbacks_list =[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40\n",
      "781/781 [==============================] - 7972s 10s/step - loss: 0.4449 - acc: 0.8478 - val_loss: 0.4478 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00031: val_acc improved from -inf to 0.84710, saving model to best_model_r3.hdf5\n",
      "Epoch 32/40\n",
      "781/781 [==============================] - 7900s 10s/step - loss: 0.4403 - acc: 0.8476 - val_loss: 0.4321 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.84710 to 0.85500, saving model to best_model_r3.hdf5\n",
      "Epoch 33/40\n",
      "781/781 [==============================] - 8036s 10s/step - loss: 0.4174 - acc: 0.8568 - val_loss: 0.5456 - val_acc: 0.8284\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.85500\n",
      "Epoch 34/40\n",
      "781/781 [==============================] - 7304s 9s/step - loss: 0.4423 - acc: 0.8473 - val_loss: 0.5238 - val_acc: 0.8305\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.85500\n",
      "Epoch 35/40\n",
      "781/781 [==============================] - 9818s 13s/step - loss: 0.4165 - acc: 0.8550 - val_loss: 0.5210 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.85500\n",
      "Epoch 36/40\n",
      "781/781 [==============================] - 13433s 17s/step - loss: 0.4104 - acc: 0.8582 - val_loss: 0.5807 - val_acc: 0.8187\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85500\n",
      "Epoch 37/40\n",
      "781/781 [==============================] - 13411s 17s/step - loss: 0.3939 - acc: 0.8650 - val_loss: 0.6119 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.85500\n",
      "Epoch 38/40\n",
      "781/781 [==============================] - 13630s 17s/step - loss: 0.4099 - acc: 0.8595 - val_loss: 0.4698 - val_acc: 0.8502\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.85500\n",
      "Epoch 39/40\n",
      "781/781 [==============================] - 11366s 15s/step - loss: 0.4034 - acc: 0.8608 - val_loss: 0.4309 - val_acc: 0.8551\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.85500 to 0.85510, saving model to best_model_r3.hdf5\n",
      "Epoch 40/40\n",
      "781/781 [==============================] - 7006s 9s/step - loss: 0.3898 - acc: 0.8651 - val_loss: 0.4831 - val_acc: 0.8418\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.85510\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=X_train.shape[0] // 64,\n",
    "                        validation_data=(X_test, y_test),initial_epoch=30,\n",
    "                        epochs=40, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/45\n",
      "781/781 [==============================] - 7039s 9s/step - loss: 0.3832 - acc: 0.8677 - val_loss: 0.7079 - val_acc: 0.7862\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.85510\n",
      "Epoch 42/45\n",
      "781/781 [==============================] - 7041s 9s/step - loss: 0.3794 - acc: 0.8668 - val_loss: 0.4529 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.85510\n",
      "Epoch 43/45\n",
      "781/781 [==============================] - 8605s 11s/step - loss: 0.3727 - acc: 0.8716 - val_loss: 0.4276 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.85510 to 0.85710, saving model to best_model_r3.hdf5\n",
      "Epoch 44/45\n",
      "781/781 [==============================] - 7732s 10s/step - loss: 0.3727 - acc: 0.8707 - val_loss: 0.4130 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.85710 to 0.86460, saving model to best_model_r3.hdf5\n",
      "Epoch 45/45\n",
      "781/781 [==============================] - 7060s 9s/step - loss: 0.3565 - acc: 0.8773 - val_loss: 0.4819 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.86460\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=X_train.shape[0] // 64,\n",
    "                        validation_data=(X_test, y_test),initial_epoch=40,\n",
    "                        epochs=45, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "781/781 [==============================] - 7031s 9s/step - loss: 0.3610 - acc: 0.8754 - val_loss: 0.4859 - val_acc: 0.8376\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.86460\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - 7041s 9s/step - loss: 0.3566 - acc: 0.8763 - val_loss: 0.4703 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.86460\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - 7025s 9s/step - loss: 0.3460 - acc: 0.8806 - val_loss: 0.4636 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.86460\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - 7160s 9s/step - loss: 0.3531 - acc: 0.8770 - val_loss: 0.4924 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.86460\n",
      "Epoch 50/50\n",
      "781/781 [==============================] - 7041s 9s/step - loss: 0.3465 - acc: 0.8787 - val_loss: 0.4686 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.86460\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=X_train.shape[0] // 64,\n",
    "                        validation_data=(X_test, y_test),initial_epoch=45,\n",
    "                        epochs=50, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/60\n",
      "781/781 [==============================] - 7030s 9s/step - loss: 0.3279 - acc: 0.8874 - val_loss: 0.4154 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.86460\n",
      "Epoch 52/60\n",
      "781/781 [==============================] - 7008s 9s/step - loss: 0.3430 - acc: 0.8818 - val_loss: 0.3911 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00052: val_acc improved from 0.86460 to 0.87100, saving model to best_model_r3.hdf5\n",
      "Epoch 53/60\n",
      "781/781 [==============================] - 7009s 9s/step - loss: 0.3316 - acc: 0.8855 - val_loss: 0.4665 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87100\n",
      "Epoch 54/60\n",
      "781/781 [==============================] - 7011s 9s/step - loss: 0.3328 - acc: 0.8844 - val_loss: 0.4882 - val_acc: 0.8448\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87100\n",
      "Epoch 55/60\n",
      "781/781 [==============================] - 7009s 9s/step - loss: 0.3172 - acc: 0.8879 - val_loss: 0.4811 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87100\n",
      "Epoch 56/60\n",
      "781/781 [==============================] - 7013s 9s/step - loss: 0.3288 - acc: 0.8852 - val_loss: 0.4762 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87100\n",
      "Epoch 57/60\n",
      "781/781 [==============================] - 7015s 9s/step - loss: 0.3173 - acc: 0.8884 - val_loss: 0.5271 - val_acc: 0.8373\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.87100\n",
      "Epoch 58/60\n",
      "781/781 [==============================] - 6990s 9s/step - loss: 0.3162 - acc: 0.8904 - val_loss: 0.5216 - val_acc: 0.8401\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87100\n",
      "Epoch 59/60\n",
      "781/781 [==============================] - 6984s 9s/step - loss: 0.3104 - acc: 0.8904 - val_loss: 0.4355 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87100\n",
      "Epoch 60/60\n",
      "781/781 [==============================] - 6991s 9s/step - loss: 0.3093 - acc: 0.8940 - val_loss: 0.3860 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.87100 to 0.87580, saving model to best_model_r3.hdf5\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=X_train.shape[0] // 64,\n",
    "                        validation_data=(X_test, y_test),initial_epoch=50,\n",
    "                        epochs=60, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/75\n",
      "781/781 [==============================] - 6984s 9s/step - loss: 0.2997 - acc: 0.8966 - val_loss: 0.4515 - val_acc: 0.8562\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.87580\n",
      "Epoch 62/75\n",
      "781/781 [==============================] - 7075s 9s/step - loss: 0.3074 - acc: 0.8953 - val_loss: 0.4174 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.87580\n",
      "Epoch 63/75\n",
      "781/781 [==============================] - 6977s 9s/step - loss: 0.2914 - acc: 0.8994 - val_loss: 0.4514 - val_acc: 0.8603\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.87580\n",
      "Epoch 64/75\n",
      "781/781 [==============================] - 6978s 9s/step - loss: 0.3007 - acc: 0.8932 - val_loss: 0.4122 - val_acc: 0.8678\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.87580\n",
      "Epoch 65/75\n",
      "781/781 [==============================] - 6973s 9s/step - loss: 0.2927 - acc: 0.8968 - val_loss: 0.4490 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.87580\n",
      "Epoch 66/75\n",
      "781/781 [==============================] - 6980s 9s/step - loss: 0.2934 - acc: 0.8980 - val_loss: 0.4019 - val_acc: 0.8717\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.87580\n",
      "Epoch 67/75\n",
      "781/781 [==============================] - 6981s 9s/step - loss: 0.2900 - acc: 0.8999 - val_loss: 0.4176 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.87580\n",
      "Epoch 68/75\n",
      "781/781 [==============================] - 7059s 9s/step - loss: 0.2888 - acc: 0.9004 - val_loss: 0.3932 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.87580\n",
      "Epoch 69/75\n",
      "781/781 [==============================] - 7219s 9s/step - loss: 0.2758 - acc: 0.9026 - val_loss: 0.4574 - val_acc: 0.8539\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.87580\n",
      "Epoch 70/75\n",
      "781/781 [==============================] - 7018s 9s/step - loss: 0.2956 - acc: 0.9000 - val_loss: 0.4375 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.87580\n",
      "Epoch 71/75\n",
      "781/781 [==============================] - 7017s 9s/step - loss: 0.2775 - acc: 0.9037 - val_loss: 0.3528 - val_acc: 0.8878\n",
      "\n",
      "Epoch 00071: val_acc improved from 0.87580 to 0.88780, saving model to best_model_r3.hdf5\n",
      "Epoch 72/75\n",
      "781/781 [==============================] - 7017s 9s/step - loss: 0.2790 - acc: 0.9036 - val_loss: 0.4473 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.88780\n",
      "Epoch 73/75\n",
      "781/781 [==============================] - 7022s 9s/step - loss: 0.2753 - acc: 0.9035 - val_loss: 0.5104 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.88780\n",
      "Epoch 74/75\n",
      "781/781 [==============================] - 7500s 10s/step - loss: 0.2762 - acc: 0.9038 - val_loss: 0.4124 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.88780\n",
      "Epoch 75/75\n",
      "781/781 [==============================] - 7759s 10s/step - loss: 0.2669 - acc: 0.9053 - val_loss: 0.3863 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.88780\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=X_train.shape[0] // 64,\n",
    "                        validation_data=(X_test, y_test),initial_epoch=60,\n",
    "                        epochs=75, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "781/781 [==============================] - 9327s 12s/step - loss: 0.2564 - acc: 0.9107 - val_loss: 0.4065 - val_acc: 0.8749\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88780\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 7234s 9s/step - loss: 0.2820 - acc: 0.9006 - val_loss: 0.3394 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00077: val_acc improved from 0.88780 to 0.88860, saving model to best_model_r3.hdf5\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 6954s 9s/step - loss: 0.2556 - acc: 0.9107 - val_loss: 0.3949 - val_acc: 0.8769\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88860\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 7318s 9s/step - loss: 0.2636 - acc: 0.9102 - val_loss: 0.3464 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88860\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 9067s 12s/step - loss: 0.2565 - acc: 0.9109 - val_loss: 0.3577 - val_acc: 0.8852\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.88860\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 8527s 11s/step - loss: 0.2618 - acc: 0.9101 - val_loss: 0.3647 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.88860\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 7042s 9s/step - loss: 0.2448 - acc: 0.9156 - val_loss: 0.3465 - val_acc: 0.8920\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.88860 to 0.89200, saving model to best_model_r3.hdf5\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 7048s 9s/step - loss: 0.2556 - acc: 0.9116 - val_loss: 0.3948 - val_acc: 0.8813\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89200\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 7044s 9s/step - loss: 0.2438 - acc: 0.9142 - val_loss: 0.3962 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89200\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 7049s 9s/step - loss: 0.2529 - acc: 0.9127 - val_loss: 0.4129 - val_acc: 0.8759\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89200\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 7310s 9s/step - loss: 0.2402 - acc: 0.9175 - val_loss: 0.3741 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89200\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 10102s 13s/step - loss: 0.2505 - acc: 0.9134 - val_loss: 0.3457 - val_acc: 0.8899\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89200\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 10712s 14s/step - loss: 0.2433 - acc: 0.9155 - val_loss: 0.4539 - val_acc: 0.8620\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89200\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 7149s 9s/step - loss: 0.2495 - acc: 0.9142 - val_loss: 0.4800 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89200\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 7172s 9s/step - loss: 0.2370 - acc: 0.9165 - val_loss: 0.4494 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89200\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 7456s 10s/step - loss: 0.2377 - acc: 0.9187 - val_loss: 0.3214 - val_acc: 0.8954\n",
      "\n",
      "Epoch 00091: val_acc improved from 0.89200 to 0.89540, saving model to best_model_r3.hdf5\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 7374s 9s/step - loss: 0.2340 - acc: 0.9189 - val_loss: 0.3748 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89540\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 7854s 10s/step - loss: 0.2424 - acc: 0.9152 - val_loss: 0.4013 - val_acc: 0.8773\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89540\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 7851s 10s/step - loss: 0.2255 - acc: 0.9221 - val_loss: 0.4338 - val_acc: 0.8654\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89540\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 7321s 9s/step - loss: 0.2288 - acc: 0.9205 - val_loss: 0.3716 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89540\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 12978s 17s/step - loss: 0.2280 - acc: 0.9214 - val_loss: 0.3624 - val_acc: 0.8884\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89540\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 11712s 15s/step - loss: 0.2298 - acc: 0.9189 - val_loss: 0.4124 - val_acc: 0.8778\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89540\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 6977s 9s/step - loss: 0.2137 - acc: 0.9270 - val_loss: 0.3714 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89540\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 6983s 9s/step - loss: 0.2317 - acc: 0.9183 - val_loss: 0.4018 - val_acc: 0.8798\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89540\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 7871s 10s/step - loss: 0.2180 - acc: 0.9241 - val_loss: 0.4224 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89540\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=X_train.shape[0] // 64,\n",
    "                        validation_data=(X_test, y_test),initial_epoch=75,\n",
    "                        epochs=100, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/110\n",
      "781/781 [==============================] - 7854s 10s/step - loss: 0.2070 - acc: 0.9281 - val_loss: 0.3600 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89540\n",
      "Epoch 102/110\n",
      "781/781 [==============================] - 7451s 10s/step - loss: 0.2288 - acc: 0.9207 - val_loss: 0.3849 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89540\n",
      "Epoch 103/110\n",
      "781/781 [==============================] - 7383s 9s/step - loss: 0.2245 - acc: 0.9205 - val_loss: 0.3560 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89540\n",
      "Epoch 104/110\n",
      "781/781 [==============================] - 8090s 10s/step - loss: 0.2169 - acc: 0.9233 - val_loss: 0.3737 - val_acc: 0.8849\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89540\n",
      "Epoch 105/110\n",
      "781/781 [==============================] - 7275s 9s/step - loss: 0.2098 - acc: 0.9263 - val_loss: 0.3889 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89540\n",
      "Epoch 106/110\n",
      "781/781 [==============================] - 7985s 10s/step - loss: 0.2155 - acc: 0.9237 - val_loss: 0.3556 - val_acc: 0.8885\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89540\n",
      "Epoch 107/110\n",
      "781/781 [==============================] - 7330s 9s/step - loss: 0.2150 - acc: 0.9243 - val_loss: 0.4620 - val_acc: 0.8666\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89540\n",
      "Epoch 108/110\n",
      "781/781 [==============================] - 7482s 10s/step - loss: 0.2163 - acc: 0.9233 - val_loss: 0.4970 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89540\n",
      "Epoch 109/110\n",
      "781/781 [==============================] - 12013s 15s/step - loss: 0.1964 - acc: 0.9322 - val_loss: 0.3212 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00109: val_acc improved from 0.89540 to 0.90060, saving model to best_model_r3.hdf5\n",
      "Epoch 110/110\n",
      "781/781 [==============================] - 13526s 17s/step - loss: 0.2123 - acc: 0.9259 - val_loss: 0.3393 - val_acc: 0.8954\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.90060\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=X_train.shape[0] // 64,\n",
    "                        validation_data=(X_test, y_test),initial_epoch=100,\n",
    "                        epochs=110, verbose=1,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('best_model_r3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1393s 139ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3211979459583759, 0.9006]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([classes[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([classes[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred        airplane  automobile  bird  cat  deer  dog  frog  horse  ship  \\\n",
      "True                                                                        \n",
      "airplane         933           6    12    7     2    0     7      4    17   \n",
      "automobile         2         982     1    0     0    0     1      0     2   \n",
      "bird              32           3   854   15    25    9    45      8     4   \n",
      "cat                7          10    35  795    20   63    47      7     4   \n",
      "deer               8           2    20   11   905   10    28     14     1   \n",
      "dog                4           2    22  100    25  800    30     13     0   \n",
      "frog               3           2    11    7     3    0   968      1     2   \n",
      "horse              9           3    14   12    16   22     4    917     0   \n",
      "ship              33          16     4    1     1    0     4      2   926   \n",
      "truck              7          55     2    0     0    0     2      1     7   \n",
      "\n",
      "Pred        truck  \n",
      "True               \n",
      "airplane       12  \n",
      "automobile     12  \n",
      "bird            5  \n",
      "cat            12  \n",
      "deer            1  \n",
      "dog             4  \n",
      "frog            3  \n",
      "horse           3  \n",
      "ship           13  \n",
      "truck         926  \n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "\n",
    "print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Used image augmentation techniques on the data set to classify the classes.\n",
    "\n",
    "2. On average each epoch took 2hrs to train(might be due to low GPUs) .\n",
    "\n",
    "3. On the 109th epoch we have got test accuracy more that 90%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
